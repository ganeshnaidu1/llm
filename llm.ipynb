{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "e63cd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "b02c9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.encoding = tiktoken.get_encoding(model_name)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        return self.encoding.encode(text)\n",
    "\n",
    "    def decode(self, tokens: list[int]):\n",
    "        return self.encoding.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "5d272e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_name\": \"cl100k_base\",\n",
    "    \"vocab_size\": 100277,\n",
    "    \"context_length\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"hidden_dim\": 3072,\n",
    "    \"embedding_dim\": 768}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "131024b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, model_name: str, config):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.tokenizer = Tokenizer(model_name) \n",
    "        self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"embedding_dim\"])\n",
    "        self.positional_encoding = nn.Embedding(config[\"context_length\"], config[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: token IDs, shape (batch, seq_len) or (seq_len,)\n",
    "        ey = self.embedding(inputs)\n",
    "        \n",
    "        # Create position indices: [0, 1, 2, 3, ..., seq_len-1]\n",
    "        seq_len = inputs.shape[-1]\n",
    "        positions = torch.arange(seq_len, dtype=torch.long)\n",
    "        pex = self.positional_encoding(positions)\n",
    "        \n",
    "        x = ey + pex\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "aa74a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.q = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        self.k = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        self.v = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        self.out_proj = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Ensure input_text has the correct shape (T, D)\n",
    "        if input_text.dim() == 2:  # If input_text is 2D (T, D)\n",
    "            text = input_text.unsqueeze(0)  # Add batch dimension (1, T, D)\n",
    "        else:\n",
    "            text = input_text\n",
    "        \n",
    "        B, T, D = text.shape\n",
    "        H = self.config[\"n_heads\"]\n",
    "        d_head = D // H\n",
    "\n",
    "        Q = self.q(text)\n",
    "        K = self.k(text)\n",
    "        V = self.v(text)\n",
    "\n",
    "        Q = Q.view(B, T, H, d_head).transpose(1, 2)\n",
    "        K = K.view(B, T, H, d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, H, d_head).transpose(1, 2)\n",
    "\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (d_head ** 0.5)\n",
    "        \n",
    "        # Apply causal mask BEFORE softmax\n",
    "        mask = torch.tril(torch.ones(T, T)).unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        out = weights @ V                # (B, H, T, d_head)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "3b68bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "d0bbead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config[\"embedding_dim\"], config[\"hidden_dim\"])\n",
    "        self.fc2 = nn.Linear(config[\"hidden_dim\"], config[\"embedding_dim\"])\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "dbd8477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.0:\n",
    "            return x\n",
    "        mask = (torch.rand_like(x) > self.p).float()\n",
    "        return x * mask / (1.0 - self.p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "6b3d3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llm(nn.Module):\n",
    "    def __init__(self, model_name: str, config):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(model_name, config)\n",
    "        self.transformer = MultiHeadAttention(config)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.layernorm1 = LayerNorm(config[\"embedding_dim\"])\n",
    "        self.layernorm2 = LayerNorm(config[\"embedding_dim\"])\n",
    "        self.feedforward = FeedForward(config)\n",
    "        self.output_layer = nn.Linear(config[\"embedding_dim\"], config[\"vocab_size\"])\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Embedding layer\n",
    "        embedded_text = self.embedding.forward(input_text)\n",
    "        \n",
    "        # First transformer block\n",
    "        x = self.transformer.forward(embedded_text)\n",
    "        x = self.dropout(x)\n",
    "        # Fix shape mismatch: remove batch dimension for residual connection\n",
    "        x = x.squeeze(0) + embedded_text  # residual connection\n",
    "        x = self.layernorm1.forward(x)\n",
    "        \n",
    "        # Feed forward block\n",
    "        residual = x\n",
    "        x = self.feedforward.forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual  # residual connection\n",
    "        x = self.layernorm2.forward(x)\n",
    "        # Output layer to convert back to vocabulary\n",
    "        logits = self.output_layer(x)\n",
    "        return logits        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "896ace2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=Llm(\"cl100k_base\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "74924f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    ###Input batch:\n",
    " ###tensor([[6109, 3626, 6100,  345],\n",
    "        ##[6109, 1110, 6622,  257]])\n",
    "    idx=idx\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            # Remove batch dimension for model (it expects single sequence)\n",
    "            logits = model(idx_cond[0])  # Pass (n_tokens,) â†’ get (n_tokens, vocab_size)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (n_tokens, vocab_size) becomes (vocab_size,)\n",
    "        logits = logits[-1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (vocab_size,)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1)  # scalar\n",
    "        \n",
    "        # Reshape to (1, 1) for concatenation\n",
    "        idx_next = idx_next.unsqueeze(0).unsqueeze(0)  # (1, 1)\n",
    "        \n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "51f63488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9906, 11, 1268, 527, 499, 30]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding_layer = Embedding(\"cl100k_base\", config)\n",
    "input_text = \"Hello, how are you?\"\n",
    "token=Tokenizer(\"cl100k_base\")\n",
    "input_text=token.encode(input_text)\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "3fd98e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?_gui linkingWizard lock vanished municipality"
     ]
    }
   ],
   "source": [
    "out = generate_text_simple(\n",
    "model=llm,\n",
    "idx=torch.tensor([input_text], dtype=torch.long),\n",
    "max_new_tokens=6,\n",
    "context_size=config[\"context_length\"]\n",
    ")\n",
    "\n",
    "for i in range(out.shape[1]):\n",
    "    print(token.decode([out[0, i].item()]), end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
