{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e63cd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b02c9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.encoding = tiktoken.get_encoding(model_name)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        return self.encoding.encode(text)\n",
    "\n",
    "    def decode(self, tokens: list[int]):\n",
    "        return self.encoding.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5d272e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_name\": \"gpt2\",\n",
    "    \"vocab_size\": 50257,  # GPT-2 vocab size\n",
    "    \"context_length\": 1024,  # GPT-2 max position\n",
    "    \"n_heads\": 12,\n",
    "    \"hidden_dim\": 3072,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"n_layers\": 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "131024b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, model_name: str, config):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.tokenizer = Tokenizer(model_name) \n",
    "        self.embedding = nn.Embedding(config[\"vocab_size\"], config[\"embedding_dim\"])\n",
    "        self.positional_encoding = nn.Embedding(config[\"context_length\"], config[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: token IDs, shape (batch, seq_len) or (seq_len,)\n",
    "        ey = self.embedding(inputs)\n",
    "        \n",
    "        # Create position indices: [0, 1, 2, 3, ..., seq_len-1]\n",
    "        seq_len = inputs.shape[-1]\n",
    "        positions = torch.arange(seq_len, dtype=torch.long)\n",
    "        pex = self.positional_encoding(positions)\n",
    "        \n",
    "        x = ey + pex\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "aa74a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.q = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        self.k = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        self.v = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "        self.out_proj = nn.Linear(config[\"embedding_dim\"], config[\"embedding_dim\"])\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Ensure input_text has the correct shape (T, D)\n",
    "        if input_text.dim() == 2:  # If input_text is 2D (T, D)\n",
    "            text = input_text.unsqueeze(0)  # Add batch dimension (1, T, D)\n",
    "        else:\n",
    "            text = input_text\n",
    "        \n",
    "        B, T, D = text.shape\n",
    "        H = self.config[\"n_heads\"]\n",
    "        d_head = D // H\n",
    "\n",
    "        Q = self.q(text)\n",
    "        K = self.k(text)\n",
    "        V = self.v(text)\n",
    "\n",
    "        Q = Q.view(B, T, H, d_head).transpose(1, 2)\n",
    "        K = K.view(B, T, H, d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, H, d_head).transpose(1, 2)\n",
    "\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (d_head ** 0.5)\n",
    "        \n",
    "        # Apply causal mask BEFORE softmax\n",
    "        mask = torch.tril(torch.ones(T, T)).unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        out = weights @ V                # (B, H, T, d_head)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3b68bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d0bbead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config[\"embedding_dim\"], config[\"hidden_dim\"])\n",
    "        self.fc2 = nn.Linear(config[\"hidden_dim\"], config[\"embedding_dim\"])\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "dbd8477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.0:\n",
    "            return x\n",
    "        mask = (torch.rand_like(x) > self.p).float()\n",
    "        return x * mask / (1.0 - self.p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6b3d3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llm(nn.Module):\n",
    "    def __init__(self, model_name: str, config):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(model_name, config)\n",
    "        self.n_layers = config[\"n_layers\"]\n",
    "        self.transformers = nn.ModuleList([MultiHeadAttention(config) for _ in range(self.n_layers)])\n",
    "        self.feedforwards = nn.ModuleList([FeedForward(config) for _ in range(self.n_layers)])\n",
    "        self.layernorms1 = nn.ModuleList([LayerNorm(config[\"embedding_dim\"]) for _ in range(self.n_layers)])\n",
    "        self.layernorms2 = nn.ModuleList([LayerNorm(config[\"embedding_dim\"]) for _ in range(self.n_layers)])\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(p=0.1) for _ in range(self.n_layers)])\n",
    "        self.output_layer = nn.Linear(config[\"embedding_dim\"], config[\"vocab_size\"])\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(input_text)\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            # Post-LN Attention\n",
    "            residual = x\n",
    "            attn_in = self.layernorms1[i](x)\n",
    "            attn_out = self.transformers[i](attn_in)\n",
    "            x = attn_out + residual\n",
    "            \n",
    "            # Post-LN FFN\n",
    "            residual = x\n",
    "            ff_in = self.layernorms2[i](x)\n",
    "            ff_out = self.feedforwards[i](ff_in)\n",
    "            x = ff_out + residual\n",
    "        \n",
    "        # Output layer to convert back to vocabulary\n",
    "        logits = self.output_layer(x)\n",
    "        return logits        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "896ace2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for 12 layers\n",
      "Loading layer 0\n",
      "Loading layer 1\n",
      "Loading layer 2\n",
      "Loading layer 3\n",
      "Loading layer 4\n",
      "Loading layer 5\n",
      "Loading layer 6\n",
      "Loading layer 7\n",
      "Loading layer 8\n",
      "Loading layer 9\n",
      "Loading layer 10\n",
      "Loading layer 11\n",
      "GPT-2 weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create your custom model and load GPT-2 weights\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "llm = Llm(\"gpt2\", config)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "def load_gpt2_weights(custom_model, gpt2_model):\n",
    "    print(f\"Loading weights for {custom_model.n_layers} layers\")\n",
    "    \n",
    "    # Load embedding weights\n",
    "    custom_model.embedding.embedding.load_state_dict({'weight': gpt2_model.transformer.wte.weight})\n",
    "    custom_model.embedding.positional_encoding.load_state_dict({'weight': gpt2_model.transformer.wpe.weight})\n",
    "    \n",
    "    # Load weights for each layer\n",
    "    for i in range(custom_model.n_layers):\n",
    "        print(f\"Loading layer {i}\")\n",
    "        # GPT-2 combines Q, K, V in c_attn: [768, 2304] = [768, 768*3]\n",
    "        c_attn_weight = gpt2_model.transformer.h[i].attn.c_attn.weight  # [768, 2304]\n",
    "        c_attn_bias = gpt2_model.transformer.h[i].attn.c_attn.bias      # [2304]\n",
    "        \n",
    "        # Split into Q, K, V weights (each [768, 768]) and transpose for PyTorch Linear\n",
    "        q_weight = c_attn_weight[:, :768].t()      # [768, 768]\n",
    "        k_weight = c_attn_weight[:, 768:1536].t()  # [768, 768] \n",
    "        v_weight = c_attn_weight[:, 1536:].t()     # [768, 768]\n",
    "        \n",
    "        q_bias = c_attn_bias[:768]             # [768]\n",
    "        k_bias = c_attn_bias[768:1536]         # [768]\n",
    "        v_bias = c_attn_bias[1536:]            # [768]\n",
    "        \n",
    "        custom_model.transformers[i].q.load_state_dict({'weight': q_weight, 'bias': q_bias})\n",
    "        custom_model.transformers[i].k.load_state_dict({'weight': k_weight, 'bias': k_bias})\n",
    "        custom_model.transformers[i].v.load_state_dict({'weight': v_weight, 'bias': v_bias})\n",
    "        \n",
    "        # Load output projection\n",
    "        custom_model.transformers[i].out_proj.load_state_dict({\n",
    "            'weight': gpt2_model.transformer.h[i].attn.c_proj.weight.t(),  # [768, 768]\n",
    "            'bias': gpt2_model.transformer.h[i].attn.c_proj.bias       # [768]\n",
    "        })\n",
    "        \n",
    "        # Load feedforward weights (need to transpose)\n",
    "        custom_model.feedforwards[i].fc1.load_state_dict({\n",
    "            'weight': gpt2_model.transformer.h[i].mlp.c_fc.weight.t(),  # [3072, 768]\n",
    "            'bias': gpt2_model.transformer.h[i].mlp.c_fc.bias\n",
    "        })\n",
    "        custom_model.feedforwards[i].fc2.load_state_dict({\n",
    "            'weight': gpt2_model.transformer.h[i].mlp.c_proj.weight.t(),  # [768, 3072]\n",
    "            'bias': gpt2_model.transformer.h[i].mlp.c_proj.bias\n",
    "        })\n",
    "        \n",
    "        # Load layer norm weights\n",
    "        custom_model.layernorms1[i].load_state_dict({\n",
    "            'gamma': gpt2_model.transformer.h[i].ln_1.weight,\n",
    "            'beta': gpt2_model.transformer.h[i].ln_1.bias\n",
    "        })\n",
    "        custom_model.layernorms2[i].load_state_dict({\n",
    "            'gamma': gpt2_model.transformer.h[i].ln_2.weight,\n",
    "            'beta': gpt2_model.transformer.h[i].ln_2.bias\n",
    "        })\n",
    "    \n",
    "    # Load output layer weights (language modeling head)\n",
    "    custom_model.output_layer.load_state_dict({\n",
    "        'weight': gpt2_model.lm_head.weight\n",
    "    }, strict=False)  # GPT-2 LM head has no bias, so use strict=False\n",
    "    \n",
    "    print(\"GPT-2 weights loaded successfully!\")\n",
    "    return custom_model\n",
    "\n",
    "# Load the weights into the llm instance\n",
    "llm = load_gpt2_weights(llm, gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "74924f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size, temperature=1.0):\n",
    "    model.eval()  # Disable dropout for inference\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)  # (batch, n_tokens, vocab_size)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        logits = logits[:, -1, :]  # (batch, vocab_size)\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "        \n",
    "        # Sample from the distribution instead of argmax\n",
    "        idx_next = torch.multinomial(probas, num_samples=1)  # (batch, 1)\n",
    "        \n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "51f63488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 703, 389, 345, 30]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding_layer = Embedding(\"gpt-2\", config)\n",
    "input_text = \"Hello, how are you?\"\n",
    "token=Tokenizer(\"gpt2\")\n",
    "input_text=token.encode(input_text)\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3fd98e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? the the the the the the"
     ]
    }
   ],
   "source": [
    "out = generate_text_simple(\n",
    "model=llm,\n",
    "idx=torch.tensor([input_text], dtype=torch.long),\n",
    "max_new_tokens=6,\n",
    "context_size=config[\"context_length\"],\n",
    "temperature=1.2\n",
    ")\n",
    "\n",
    "for i in range(out.shape[1]):\n",
    "    print(token.decode([out[0, i].item()]), end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
